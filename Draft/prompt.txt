Prompt:


Write a Python script that reads multiple JSON files named instance_i.json in the current directory and extracts _source >> extractionMetadata >> texts >> title and _source >> extractionMetadata >> texts >> value. For each JSON file, create a directory named after the file (e.g., ./instance_82 for instance_82.json) and generate separate text files for each section in texts within that directory. Each text file should contain the title and value. Mimic the logic from the provided code example which processes tables and technologyResults:

def extract_typing_labels(json_file):
	with open(json_file, 'r', encoding='utf-8') as file:
    	data = json.load(file)
    
	results = []
	logs = []

	extraction_metadata = data["_source"]["extractionMetadata"]
	preprocessing_metadata = data["_source"]["preprocessingMetadata"]
    
	tables = extraction_metadata[0]["tables"]
	tech_results = preprocessing_metadata[0]["technologyResults"]
    
	tech_results_dict = {tr["index"]: tr for tr in tech_results}
    
	for table in tables:
    	table_num = table["tableNum"]
    	print(f"Processing tableNum: {table_num}")
    	if table_num in tech_results_dict:
        	tech_result = tech_results_dict[table_num]
        	if "dagobah" in tech_result and "preprocessed" in tech_result["dagobah"]:
            	if "primitiveTyping" in tech_result["dagobah"]["preprocessed"]:
                	primitive_typing = tech_result["dagobah"]["preprocessed"]["primitiveTyping"]
                	max_column_index = max([col["columnIndex"] for col in primitive_typing]) + 1
                	column_types = ["UNKNOWN"] * max_column_index
                	for column in primitive_typing:
                    	column_index = column["columnIndex"]
                    	if column["typing"]:
                        	column_type = column["typing"][0]["typingLabel"]
                        	if column_type == 'DATE':
                            	column_type = 'datetime'
                        	column_types[column_index] = column_type
                	results.append([table_num, max_column_index] + column_types)




***

Write a Python script that reads a JSON file and outputs a CSV file containing a table with the following structure:

    The row header should be "TableNum".
    The column headers should be "Column 1", "Column 2", ..., up to the maximum number of columns determined dynamically from the JSON data.

Each cell in the table represents the "typingLabel" for a column in the original tables, identified by the [tableNum, columnIndex] coordinate. If there are multiple "typingLabel" values for a column, always choose the first one.

Structure Details:

The JSON structure is as follows:

    _source >> extractionMetadata >> tables >> tableNum
    _source >> preprocessingMetadata >> technologyResults >> dagobah >> index
    _source >> preprocessingMetadata >> technologyResults >> dagobah >> preprocessed >> primitiveTyping >> columnIndex
    _source >> preprocessingMetadata >> technologyResults >> dagobah >> preprocessed >> primitiveTyping >> typing >> typingLabel

    Match tableNum from tables with index in technologyResults to find the corresponding typing information.

    Ensure that the number of columns in the CSV corresponds to the highest columnIndex found in the primitiveTyping data for each table.

    Create a log file recording any tables for which no primitiveTyping data was found.

Example of the CSV structure:

sql

| TableNum | Column 1 | Column 2 | Column 3 | Column 4 |
|----------|----------|----------|----------|----------|
| 0        | PERSON   | UNKNOWN  | UNKNOWN  | UNKNOWN  |

Output Requirements:

    The CSV file should be saved as output.csv.
    The log file should be saved as log.txt with entries such as "For file [file_name], the table [tableNum] no primitiveTyping data was found."

Please write and execute the Python script.

***

extract_typing_labels: Extracts typing labels and generates a CSV file. Ensures "UNKNOWN" values are not converted to NaN.
extract_key_column: Extracts the primary key position directly from the JSON file.
extract_typing_label_from_csv: Extracts typing labels from the generated CSV files for a specific table, ensuring no conversion to NaN and removing trailing empty values.
update_table_with_key_and_types: Updates the processed JSON data with the key column and column types.
process_file: Main function to process each file, generate the CSV, log missing data, and update the JSON output.
Loop through the files: Processes files from 1 to 100.



***


Prompt:

Write a Python script that reads multiple JSON files named instance_i.json in the current directory and extracts _source >> extractionMetadata >> texts >> title and _source >> extractionMetadata >> texts >> value. For each JSON file, create a directory named after the file (e.g., ./instance_82 for instance_82.json) and generate separate text files for each section in texts within that directory. Each text file should contain the title and value. Mimic the logic from the provided code example which processes tables and technologyResults:

def extract_typing_labels(json_file):
	with open(json_file, 'r', encoding='utf-8') as file:
    	data = json.load(file)
    
	results = []
	logs = []

	extraction_metadata = data["_source"]["extractionMetadata"]
	preprocessing_metadata = data["_source"]["preprocessingMetadata"]
    
	tables = extraction_metadata[0]["tables"]
	tech_results = preprocessing_metadata[0]["technologyResults"]
    
	tech_results_dict = {tr["index"]: tr for tr in tech_results}
    
	for table in tables:
    	table_num = table["tableNum"]
    	print(f"Processing tableNum: {table_num}")
    	if table_num in tech_results_dict:
        	tech_result = tech_results_dict[table_num]
        	if "dagobah" in tech_result and "preprocessed" in tech_result["dagobah"]:
            	if "primitiveTyping" in tech_result["dagobah"]["preprocessed"]:
                	primitive_typing = tech_result["dagobah"]["preprocessed"]["primitiveTyping"]
                	max_column_index = max([col["columnIndex"] for col in primitive_typing]) + 1
                	column_types = ["UNKNOWN"] * max_column_index
                	for column in primitive_typing:
                    	column_index = column["columnIndex"]
                    	if column["typing"]:
                        	column_type = column["typing"][0]["typingLabel"]
                        	if column_type == 'DATE':
                            	column_type = 'datetime'
                        	column_types[column_index] = column_type
                	results.append([table_num, max_column_index] + column_types)



Please update the code to perform the following tasks:

    Check if the number of rows in "column_types" matches the number of rows in "header".
    If "column_types" has fewer rows than "header", fill in the missing rows with the type "string".
    If "column_types" has more rows than "header", truncate the extra rows.
    Ensure this check and adjustment happens before any further processing of the column types.
    Ensure the existing functionality for determining and refining column types based on the values in the "rows" remains intact.


***

Prompt:

I have multiple JSON files named in the pattern synthetic_qa_output_instance_i_v6.json (e.g., synthetic_qa_output_instance_29_v6.json, synthetic_qa_output_instance_32_v6.json) in the current directory. Each JSON file contains multiple tables, and each table includes a list of question-answer sets (qas).

I need a Python script that does the following:

    Reads all JSON files in the current directory that match the pattern synthetic_qa_output_instance_*_v6.json.
    For each JSON file:
        Extracts the instance index from the filename.
        Creates a directory named ./qas_instance_index (e.g., ./qas_29 for synthetic_qa_output_instance_29_v6.json).
        For each table in the JSON file:
            Extracts all qas.
            Saves the qas of each table into a separate text file within the created directory. The text file should be named qas_instance_index_table_table_index.txt (e.g., qas_29_table_1.txt).
            Each text file should contain the question and answer pairs in a human-readable format, with each question followed by its answers.


***

Optimized Prompt:

I need a Python script that iterates through each file in all subdirectories of both ./embedding/qas and ./embedding/text directories, and calculates cosine similarity between the files. The specific requirements are as follows:

    Directory Structure:
        In ./embedding/qas, there are subdirectories named qas_i (e.g., qas_29).
        Each qas_i directory contains files named qas_i_table_j.txt (e.g., qas_29_table_1.txt).
        In ./embedding/text, there are subdirectories named instance_i (e.g., instance_29).
        Each instance_i directory contains files named section_j.txt (e.g., section_1.txt).

    Task:
        For each text file in ./embedding/qas/qas_i, calculate the cosine similarity with every text file in the corresponding ./embedding/text/instance_i directory (where i is the same for both directories).
        Save the cosine similarity scores in an output directory ./embedding/output with the same subdirectory structure as the input.

    Output:
        The output for each qas_i directory should be a CSV file named cosine_similarity.csv in the corresponding ./embedding/output/qas_i directory.
        The CSV file should have three columns: Similarity, QAS_File, and Text_File.
        Each row should contain the similarity score, the name of the QAS file, and the name of the text file it was compared with.

    Additional Features:
        Include a progress bar to indicate the progress of the file processing.
        Ensure the script is efficient and handles large numbers of files gracefully.

